{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InkCYb-pKgqH"
      },
      "source": [
        "# Project Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrfCJQQXKgqI"
      },
      "source": [
        "The Film Junky Union, a new edgy community for classic movie enthusiasts, is developing a system for filtering and categorizing movie reviews. The goal is to train a model to automatically detect negative reviews. You'll be using a dataset of IMBD movie reviews with polarity labelling to build a model for classifying positive and negative reviews. It will need to have an F1 score of at least 0.85."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Colab setup (only needed in Google Colab)\n",
        "if os.path.exists(\"/content\"):\n",
        "    if not os.path.exists(\"Data_projects_TripleTen\"):\n",
        "        !git clone https://github.com/Olivenatural/Data_projects_TripleTen.git\n",
        "    %cd Data_projects_TripleTen/Sprint14_Text_ML"
      ],
      "metadata": {
        "id": "rimXDXb3Y1P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ✅ GitHub / Colab sample file\n",
        "DATA_PATH = \"data/sample/imdb_reviews_sample.tsv\"\n",
        "\n",
        "# ✅ TripleTen full dataset (uncomment if running inside TripleTen)\n",
        "# DATA_PATH = \"/datasets/imdb_reviews.tsv\"\n",
        "\n",
        "\n",
        "df_reviews = pd.read_csv(DATA_PATH, sep=\"\\t\")\n",
        "\n",
        "if \"votes\" in df_reviews.columns:\n",
        "    df_reviews[\"votes\"] = df_reviews[\"votes\"].astype(\"Int64\")\n",
        "\n",
        "df_reviews.head()"
      ],
      "metadata": {
        "id": "N0VwCNKBLcfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "repo_dir = \"Data_projects_TripleTen\"\n",
        "\n",
        "if not os.path.exists(repo_dir):\n",
        "    !git clone https://github.com/YOUR-USERNAME/Data_projects_TripleTen.git\n",
        "\n",
        "%cd Data_projects_TripleTen/Sprint14_Text_ML"
      ],
      "metadata": {
        "id": "CZ7fFYB2BlPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ5GTdpJKgqJ"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MOc5k49KgqJ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MwpHQeuKgqK"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'png'\n",
        "# the next line provides graphs of better quality on HiDPI screens\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m--n2ndMKgqK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# this is to use progress_apply, read more at https://pypi.org/project/tqdm/#pandas-integration\n",
        "tqdm.pandas()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVaKczW9KgqK"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ✅ GitHub / Colab sample file\n",
        "DATA_PATH = \"data/sample/imdb_reviews_sample.tsv\"\n",
        "\n",
        "# ✅ TripleTen full dataset (uncomment if running inside TripleTen)\n",
        "# DATA_PATH = \"/datasets/imdb_reviews.tsv\"\n",
        "\n",
        "\n",
        "DATA_PATH = \"data/sample/imdb_reviews_sample.tsv\"\n",
        "df_reviews = pd.read_csv(DATA_PATH, sep=\"\\t\")\n",
        "\n",
        "if \"votes\" in df_reviews.columns:\n",
        "    df_reviews[\"votes\"] = df_reviews[\"votes\"].astype(\"Int64\")\n",
        "\n",
        "df_reviews.head()"
      ],
      "metadata": {
        "id": "5dRfEhDLByds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns in df_reviews:\", df_reviews.columns.tolist())"
      ],
      "metadata": {
        "id": "pfmR8RXoDHzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_MODE = set(df_reviews.columns) == {\"review\", \"pos\", \"ds_part\"}\n",
        "print(\"SAMPLE_MODE:\", SAMPLE_MODE)"
      ],
      "metadata": {
        "id": "RH4au83DEr4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if SAMPLE_MODE:\n",
        "    print(\"Skipping this section in sample mode (columns not available).\")\n",
        "else:\n",
        "    # your original code that uses extra columns\n",
        "    dft1 = df_reviews[['tconst', 'start_year']].drop_duplicates()['start_year'].value_counts().sort_index()"
      ],
      "metadata": {
        "id": "80kZGMe9E5p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIYxINMOKgqL"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK4fU6VKKgqL"
      },
      "source": [
        "Let's check the number of movies and reviews over years."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "needed_cols = {\"tconst\", \"start_year\"}\n",
        "\n",
        "if needed_cols.issubset(df_reviews.columns):\n",
        "    dft1 = df_reviews[[\"tconst\", \"start_year\"]].drop_duplicates()[\"start_year\"] \\\n",
        "        .value_counts().sort_index()\n",
        "\n",
        "    dft1 = dft1.reindex(index=np.arange(dft1.index.min(), max(dft1.index.max(), 2021))).fillna(0)\n",
        "\n",
        "    ax = axs[0]  # keep your existing axes line if you already created axs\n",
        "    ax.plot(dft1.index, dft1.values)\n",
        "    ax.set_title(\"Reviews by start_year\")\n",
        "else:\n",
        "    print(\"Skipping start_year plot — sample file does not include 'tconst' and 'start_year'.\")"
      ],
      "metadata": {
        "id": "Z75R3y9lD2pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axs = plt.subplots(2, 1, figsize=(16, 8))\n",
        "\n",
        "needed_cols = {\"tconst\", \"start_year\", \"pos\"}\n",
        "\n",
        "if needed_cols.issubset(df_reviews.columns):\n",
        "    # --- Plot 1: Number of Movies Over Years ---\n",
        "    ax = axs[0]\n",
        "    dft1 = (df_reviews[[\"tconst\", \"start_year\"]]\n",
        "            .drop_duplicates()[\"start_year\"]\n",
        "            .value_counts()\n",
        "            .sort_index())\n",
        "\n",
        "    dft1 = dft1.reindex(index=np.arange(dft1.index.min(), max(dft1.index.max(), 2021))).fillna(0)\n",
        "    dft1.plot(kind=\"bar\", ax=ax)\n",
        "    ax.set_title(\"Number of Movies Over Years\")\n",
        "\n",
        "    # --- Plot 2: Number of Reviews Over Years ---\n",
        "    ax = axs[1]\n",
        "\n",
        "    dft2 = df_reviews.groupby([\"start_year\", \"pos\"])[\"pos\"].count().unstack()\n",
        "    dft2 = dft2.reindex(index=np.arange(dft2.index.min(), max(dft2.index.max(), 2021))).fillna(0)\n",
        "    dft2.plot(kind=\"bar\", stacked=True, label=\"#reviews (neg, pos)\", ax=ax)\n",
        "\n",
        "    dft_total = df_reviews[\"start_year\"].value_counts().sort_index()\n",
        "    dft_total = dft_total.reindex(index=np.arange(dft_total.index.min(), max(dft_total.index.max(), 2021))).fillna(0)\n",
        "\n",
        "    reviews_per_movie = (dft_total / dft1).fillna(0)\n",
        "    axt = ax.twinx()\n",
        "    reviews_per_movie.reset_index(drop=True).rolling(5).mean().plot(\n",
        "        color=\"orange\",\n",
        "        label=\"reviews per movie (avg over 5 years)\",\n",
        "        ax=axt\n",
        "    )\n",
        "\n",
        "    lines, labels = axt.get_legend_handles_labels()\n",
        "    ax.legend(lines, labels, loc=\"upper left\")\n",
        "    ax.set_title(\"Number of Reviews Over Years\")\n",
        "\n",
        "else:\n",
        "    # Sample dataset only has review/pos/ds_part\n",
        "    for ax in axs:\n",
        "        ax.axis(\"off\")\n",
        "    axs[0].text(\n",
        "        0.01, 0.6,\n",
        "        \"Skipped: Year-based plots require columns like 'tconst' and 'start_year'.\\n\"\n",
        "        \"Your sample file only includes: review, pos, ds_part.\",\n",
        "        fontsize=12\n",
        "    )\n",
        "\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "N60oogyUGQrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm0OjA5bKgqM"
      },
      "source": [
        "### Number of Reviews Over Years — what I’m seeing\n",
        "\n",
        "- **Volume is climbing over time.** The stacked bars get much taller in the later years, which means IMDb activity (reviews submitted) has grown a lot.\n",
        "- **Positives vs. negatives rise together.** Both sentiments increase in roughly the same proportion each year. No big long-term tilt toward one class.\n",
        "- **Reviews per movie are trending up.** The orange rolling line (avg. reviews per movie) moves upward, so newer titles tend to attract more reviews than older ones.\n",
        "- **Edge effects at the end.** The very last year or two can be under-counted if the data isn’t complete—so I won’t over-interpret the final bars.\n",
        "\n",
        "**So what (for modeling):**\n",
        "- The dataset looks **roughly balanced** by sentiment across years → I probably don’t need heavy class-imbalance tricks.\n",
        "- Language and topics may **drift over time** (new slang, streaming-era phrasing), so I’ll keep my split strict (fit vectorizer on train only) and watch for generalization on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLvcySTZKgqM"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Great job setting up the environment with all the required libraries and style configs. The EDA plots are clear, well-labeled, and your commentary ties the visual patterns directly to their modeling implications (class balance, drift, generalization).\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRNN6s7PKgqM"
      },
      "source": [
        "Let's check the distribution of number of reviews per movie with the exact counting and KDE (just to learn how it may differ from the exact counting)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "needed_cols = {\"tconst\", \"review\"}\n",
        "\n",
        "if needed_cols.issubset(df_reviews.columns):\n",
        "    ax = axs[0]\n",
        "    dft = (df_reviews.groupby(\"tconst\")[\"review\"].count()\n",
        "           .value_counts()\n",
        "           .sort_index())\n",
        "    dft.plot.bar(ax=ax)\n",
        "    ax.set_title(\"Bar Plot of #Reviews Per Movie\")\n",
        "\n",
        "    ax = axs[1]\n",
        "    dft = df_reviews.groupby(\"tconst\")[\"review\"].count()\n",
        "    sns.kdeplot(dft, ax=ax)\n",
        "    ax.set_title(\"KDE Plot of #Reviews Per Movie\")\n",
        "\n",
        "else:\n",
        "    for ax in axs:\n",
        "        ax.axis(\"off\")\n",
        "    axs[0].text(\n",
        "        0.01, 0.6,\n",
        "        \"Skipped: This plot needs the 'tconst' column.\\n\"\n",
        "        \"Your sample file only includes: review, pos, ds_part.\",\n",
        "        fontsize=12\n",
        "    )\n",
        "\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "n61gz9amG422"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR_H7vRVKgqM"
      },
      "source": [
        "### # Reviews per Movie — my notes\n",
        "\n",
        "- I see **most movies have just a handful of reviews**. The bars are tallest at 1–3 and drop off fast.\n",
        "- There’s a **long right tail** — a few titles get tons of reviews (blockbusters/cult favorites).\n",
        "- The KDE curve backs this up: big peak at low counts, then a slow taper → **very skewed** (mean > median).\n",
        "\n",
        "**What this means for my modeling**\n",
        "- A small set of popular movies could **dominate the vocabulary**. I’m keeping the vectorizer fit on **train only** to avoid leakage.\n",
        "- I’ll watch that results aren’t driven by those high-volume titles; if needed, I’ll sanity-check performance across different movies.\n",
        "- For slower experiments (like BERT samples), I’ll **sample a diverse set of movies**, not just the popular ones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEdxznv8KgqN"
      },
      "outputs": [],
      "source": [
        "df_reviews['pos'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "needed_cols = {\"ds_part\", \"rating\"}\n",
        "\n",
        "if needed_cols.issubset(df_reviews.columns):\n",
        "    ax = axs[0]\n",
        "    dft = df_reviews.query('ds_part == \"train\"')[\"rating\"].value_counts().sort_index()\n",
        "    dft = dft.reindex(index=np.arange(min(dft.index.min(), 1), max(dft.index.max(), 11))).fillna(0)\n",
        "    dft.plot.bar(ax=ax)\n",
        "    ax.set_ylim([0, 5000])\n",
        "    ax.set_title(\"The train set: distribution of ratings\")\n",
        "\n",
        "    ax = axs[1]\n",
        "    dft = df_reviews.query('ds_part == \"test\"')[\"rating\"].value_counts().sort_index()\n",
        "    dft = dft.reindex(index=np.arange(min(dft.index.min(), 1), max(dft.index.max(), 11))).fillna(0)\n",
        "    dft.plot.bar(ax=ax)\n",
        "    ax.set_ylim([0, 5000])\n",
        "    ax.set_title(\"The test set: distribution of ratings\")\n",
        "\n",
        "else:\n",
        "    for ax in axs:\n",
        "        ax.axis(\"off\")\n",
        "    axs[0].text(\n",
        "        0.01, 0.6,\n",
        "        \"Skipped: This plot needs the 'rating' column.\\n\"\n",
        "        \"Your sample file only includes: review, pos, ds_part.\",\n",
        "        fontsize=12\n",
        "    )\n",
        "\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "2QhMvtZvHfE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0oZYOP1KgqN"
      },
      "source": [
        "### Ratings distribution (train vs. test) — my notes\n",
        "\n",
        "- Both splits show the same shape: **lots of 1–2s and 8–10s**, and **fewer mid-range ratings (4–6)**. It’s that classic “U-shape” you see on IMDb.\n",
        "- The train and test charts look **very similar in scale** (y-axis capped at 5k), so there’s **no obvious distribution shift** between splits.\n",
        "- This suggests many reviews come from people with **strong opinions** (love it or hate it), while truly neutral takes are rarer.\n",
        "\n",
        "**Implications for modeling**\n",
        "- Extremes should be **easier** for the classifier; the **middle band** is where I expect more mistakes.\n",
        "- I’ll keep an eye on **errors by rating bucket** to see if performance dips in the 4–6 range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk3gmeddKgqN"
      },
      "source": [
        "Distribution of negative and positive reviews over the years for two parts of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(\n",
        "    2, 2, figsize=(16, 8),\n",
        "    gridspec_kw=dict(width_ratios=(2, 1), height_ratios=(1, 1))\n",
        ")\n",
        "\n",
        "needed_cols = {\"ds_part\", \"pos\", \"start_year\", \"tconst\"}\n",
        "\n",
        "if needed_cols.issubset(df_reviews.columns):\n",
        "    # --- Train: reviews per year (stacked) ---\n",
        "    ax = axs[0][0]\n",
        "    dft = (df_reviews.query('ds_part == \"train\"')\n",
        "           .groupby([\"start_year\", \"pos\"])[\"pos\"]\n",
        "           .count()\n",
        "           .unstack())\n",
        "    dft.index = dft.index.astype(\"int\")\n",
        "    dft = dft.reindex(index=np.arange(dft.index.min(), max(dft.index.max(), 2020))).fillna(0)\n",
        "    dft.plot(kind=\"bar\", stacked=True, ax=ax)\n",
        "    ax.set_title(\"The train set: #reviews of different polarities per year\")\n",
        "\n",
        "    # --- Train: distribution per movie (KDE) ---\n",
        "    ax = axs[0][1]\n",
        "    dft = (df_reviews.query('ds_part == \"train\"')\n",
        "           .groupby([\"tconst\", \"pos\"])[\"pos\"]\n",
        "           .count()\n",
        "           .unstack())\n",
        "\n",
        "    if 0 in dft.columns:\n",
        "        sns.kdeplot(dft[0], label=\"negative\", ax=ax)\n",
        "    if 1 in dft.columns:\n",
        "        sns.kdeplot(dft[1], label=\"positive\", ax=ax)\n",
        "\n",
        "    ax.legend()\n",
        "    ax.set_title(\"The train set: distribution of polarities per movie\")\n",
        "\n",
        "    # --- Test: reviews per year (stacked) ---\n",
        "    ax = axs[1][0]\n",
        "    dft = (df_reviews.query('ds_part == \"test\"')\n",
        "           .groupby([\"start_year\", \"pos\"])[\"pos\"]\n",
        "           .count()\n",
        "           .unstack())\n",
        "    dft.index = dft.index.astype(\"int\")\n",
        "    dft = dft.reindex(index=np.arange(dft.index.min(), max(dft.index.max(), 2020))).fillna(0)\n",
        "    dft.plot(kind=\"bar\", stacked=True, ax=ax)\n",
        "    ax.set_title(\"The test set: #reviews of different polarities per year\")\n",
        "\n",
        "    # --- Test: distribution per movie (KDE) ---\n",
        "    ax = axs[1][1]\n",
        "    dft = (df_reviews.query('ds_part == \"test\"')\n",
        "           .groupby([\"tconst\", \"pos\"])[\"pos\"]\n",
        "           .count()\n",
        "           .unstack())\n",
        "\n",
        "    if 0 in dft.columns:\n",
        "        sns.kdeplot(dft[0], label=\"negative\", ax=ax)\n",
        "    if 1 in dft.columns:\n",
        "        sns.kdeplot(dft[1], label=\"positive\", ax=ax)\n",
        "\n",
        "    ax.legend()\n",
        "    ax.set_title(\"The test set: distribution of polarities per movie\")\n",
        "\n",
        "else:\n",
        "    # Sample dataset only has review/pos/ds_part → skip this 4-plot panel cleanly\n",
        "    for row in axs:\n",
        "        for ax in row:\n",
        "            ax.axis(\"off\")\n",
        "    axs[0][0].text(\n",
        "        0.01, 0.6,\n",
        "        \"Skipped: This 4-panel analysis needs 'start_year' and 'tconst'.\\n\"\n",
        "        \"Your sample file only includes: review, pos, ds_part.\",\n",
        "        fontsize=12\n",
        "    )\n",
        "\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "YnQTDNDtIP8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGlvt_chKgqN"
      },
      "source": [
        "### Sentiment over years (train & test) — my notes\n",
        "\n",
        "- **Clear growth over time.** In both splits, the stacked bars get taller in later years, so the volume of reviews has increased a lot.\n",
        "- **Positives and negatives track together.** Both classes rise at similar rates each year; I don’t see a big year-by-year skew toward one label.\n",
        "- **Per-movie counts are skewed but similar by class.** The KDE curves (right panels) for positive vs. negative overlap heavily — most movies get few reviews, with a long tail of very popular titles.\n",
        "- **Train vs. test look aligned.** The shapes of both the yearly bars and the KDE curves are very similar between the splits, which suggests **no obvious distribution shift**.\n",
        "\n",
        "**Implications for the model**\n",
        "- Class balance looks stable over time; I don’t need heavy rebalancing tricks.\n",
        "- Because review volume changes by year, I’ll continue to **fit vectorizers on train only** and watch for any time-based drift in test performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtheE5ghKgqN"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Excellent EDA continuation — the plots on reviews per movie, rating distributions, and sentiment over time are well-structured and accompanied by thoughtful commentary. You’ve clearly explained both the patterns (right-skewed distributions, U-shaped ratings, class balance over time) and their implications for modeling, which is exactly what we’re looking for.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsFVBtQnKgqN"
      },
      "source": [
        "## Evaluation Procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae3xgn58KgqN"
      },
      "source": [
        "Composing an evaluation routine which can be used for all models in this project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZQqI_bHKgqN"
      },
      "outputs": [],
      "source": [
        "\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "def evaluate_model(model, train_features, train_target, test_features, test_target):\n",
        "\n",
        "    eval_stats = {}\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "    for type, features, target in (('train', train_features, train_target), ('test', test_features, test_target)):\n",
        "\n",
        "        eval_stats[type] = {}\n",
        "\n",
        "        pred_target = model.predict(features)\n",
        "        pred_proba = model.predict_proba(features)[:, 1]\n",
        "\n",
        "        # F1\n",
        "        f1_thresholds = np.arange(0, 1.01, 0.05)\n",
        "        f1_scores = [metrics.f1_score(target, pred_proba>=threshold) for threshold in f1_thresholds]\n",
        "\n",
        "        # ROC\n",
        "        fpr, tpr, roc_thresholds = metrics.roc_curve(target, pred_proba)\n",
        "        roc_auc = metrics.roc_auc_score(target, pred_proba)\n",
        "        eval_stats[type]['ROC AUC'] = roc_auc\n",
        "\n",
        "        # PRC\n",
        "        precision, recall, pr_thresholds = metrics.precision_recall_curve(target, pred_proba)\n",
        "        aps = metrics.average_precision_score(target, pred_proba)\n",
        "        eval_stats[type]['APS'] = aps\n",
        "\n",
        "        if type == 'train':\n",
        "            color = 'blue'\n",
        "        else:\n",
        "            color = 'green'\n",
        "\n",
        "        # F1 Score\n",
        "        ax = axs[0]\n",
        "        max_f1_score_idx = np.argmax(f1_scores)\n",
        "        ax.plot(f1_thresholds, f1_scores, color=color, label=f'{type}, max={f1_scores[max_f1_score_idx]:.2f} @ {f1_thresholds[max_f1_score_idx]:.2f}')\n",
        "        # setting crosses for some thresholds\n",
        "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
        "            closest_value_idx = np.argmin(np.abs(f1_thresholds-threshold))\n",
        "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
        "            ax.plot(f1_thresholds[closest_value_idx], f1_scores[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
        "        ax.set_xlim([-0.02, 1.02])\n",
        "        ax.set_ylim([-0.02, 1.02])\n",
        "        ax.set_xlabel('threshold')\n",
        "        ax.set_ylabel('F1')\n",
        "        ax.legend(loc='lower center')\n",
        "        ax.set_title(f'F1 Score')\n",
        "\n",
        "        # ROC\n",
        "        ax = axs[1]\n",
        "        ax.plot(fpr, tpr, color=color, label=f'{type}, ROC AUC={roc_auc:.2f}')\n",
        "        # setting crosses for some thresholds\n",
        "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
        "            closest_value_idx = np.argmin(np.abs(roc_thresholds-threshold))\n",
        "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
        "            ax.plot(fpr[closest_value_idx], tpr[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
        "        ax.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
        "        ax.set_xlim([-0.02, 1.02])\n",
        "        ax.set_ylim([-0.02, 1.02])\n",
        "        ax.set_xlabel('FPR')\n",
        "        ax.set_ylabel('TPR')\n",
        "        ax.legend(loc='lower center')\n",
        "        ax.set_title(f'ROC Curve')\n",
        "\n",
        "        # PRC\n",
        "        ax = axs[2]\n",
        "        ax.plot(recall, precision, color=color, label=f'{type}, AP={aps:.2f}')\n",
        "        # setting crosses for some thresholds\n",
        "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
        "            closest_value_idx = np.argmin(np.abs(pr_thresholds-threshold))\n",
        "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
        "            ax.plot(recall[closest_value_idx], precision[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
        "        ax.set_xlim([-0.02, 1.02])\n",
        "        ax.set_ylim([-0.02, 1.02])\n",
        "        ax.set_xlabel('recall')\n",
        "        ax.set_ylabel('precision')\n",
        "        ax.legend(loc='lower center')\n",
        "        ax.set_title(f'PRC')\n",
        "\n",
        "        eval_stats[type]['Accuracy'] = metrics.accuracy_score(target, pred_target)\n",
        "        eval_stats[type]['F1'] = metrics.f1_score(target, pred_target)\n",
        "\n",
        "    df_eval_stats = pd.DataFrame(eval_stats)\n",
        "    df_eval_stats = df_eval_stats.round(2)\n",
        "    df_eval_stats = df_eval_stats.reindex(index=('Accuracy', 'F1', 'APS', 'ROC AUC'))\n",
        "\n",
        "    print(df_eval_stats)\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_e2VXErKgqO"
      },
      "source": [
        "## Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIz1G4vVKgqO"
      },
      "source": [
        "We assume all models below accepts texts in lowercase and without any digits, punctuations marks etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQaj37IMKgqO"
      },
      "outputs": [],
      "source": [
        "df_reviews['review_norm'] = (df_reviews['review']\n",
        "                            .str.lower()\n",
        "                            .str.replace(r'<br\\s*/?>', ' ', regex=True)\n",
        "                            .str.replace(r'<[^>]+>', ' ', regex=True)\n",
        "                            .str.replace(r\"[^a-z\\s']\", ' ', regex=True)\n",
        "                            .str.replace(r'\\s+', ' ', regex=True)\n",
        "                            .str.strip()\n",
        "                            )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WibAYnW_KgqO"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Great start on text preprocessing! You’re normalizing reviews by lowercasing, removing HTML tags, non-alphabetic characters, and trimming whitespace — all important steps to clean noisy raw text. The chained `.str.replace(..., regex=True)` is clear and concise.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awnOhs-JKgqO"
      },
      "source": [
        "Next, I'm going to tell TF-IDF to keep apostrophies in tokens. Sklearn's default token pattern splits \"don't\" into \"don\" and \"t\". I can keep it as one token by setting token_pattern:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR71To81KgqO"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    ngram_range=(1,2),\n",
        "    min_df=3, max_df=0.9,\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    token_pattern=r\"(?u)\\b[a-z][a-z']+\\b\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dqdkcyxKgqO"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Excellent adjustment on the TF-IDF vectorizer! By customizing `token_pattern` to keep apostrophes, you ensure contractions like “don’t” or “can’t” stay intact, which preserves sentiment-carrying tokens. Good use of other parameters too (`ngram_range`, `min_df`, `max_df`, `sublinear_tf`, `strip_accents`), showing awareness of both signal strength and noise control.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe5_rYd6KgqO"
      },
      "source": [
        "## Train / Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxKdW4YDKgqO"
      },
      "source": [
        "Luckily, the whole dataset is already divided into train/test one parts. The corresponding flag is 'ds_part'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFPwyV-9KgqO"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_reviews_train = df_reviews.query('ds_part == \"train\"').copy()\n",
        "df_reviews_test = df_reviews.query('ds_part == \"test\"').copy()\n",
        "\n",
        "train_target = df_reviews_train['pos']\n",
        "test_target = df_reviews_test['pos']\n",
        "\n",
        "print(df_reviews_train.shape)\n",
        "print(df_reviews_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi31wleEKgqO"
      },
      "source": [
        "## Working with models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U8x1ZgIKgqO"
      },
      "source": [
        "### Model 0 - Constant Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzCfr5rgKgqO"
      },
      "source": [
        "A trivial classifier that predicts the same class every time. I'll set contant=0(always \"negative\"). This should produce F1=0 for the positive class and gives me a foundation to beat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJHM44aZKgqT"
      },
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR2uxW1AKgqU"
      },
      "outputs": [],
      "source": [
        "# 1) Text features\n",
        "features_train_text = df_reviews_train['review_norm']\n",
        "features_test_text = df_reviews_test['review_norm']\n",
        "target_train = train_target.astype(int)\n",
        "target_test = test_target.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z_kQB5YKgqU"
      },
      "outputs": [],
      "source": [
        "# 2) Vectorize\n",
        "tfidf_const = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=3, max_df=0.9,\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode'\n",
        ")\n",
        "features_train_const = tfidf_const.fit_transform(features_train_text)\n",
        "features_test_const = tfidf_const.transform(features_test_text)\n",
        "\n",
        "# 3) Dummy Classifier: \"constant\"\n",
        "dummy_constant = DummyClassifier(strategy='constant', constant=0, random_state=42)\n",
        "dummy_constant.fit(features_train_const, target_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCZ-HbQFKgqU"
      },
      "outputs": [],
      "source": [
        "# 4) Evaluation\n",
        "evaluate_model(dummy_constant, features_train_const, target_train, features_test_const, target_test);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULIRiughKgqU"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Great work establishing a baseline with `DummyClassifier` and immediately evaluating it using your `evaluate_model()` function. This ensures you have a clear benchmark to compare future models against. Nice job also keeping the vectorizer fit on train only, then transforming test separately — that prevents data leakage.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0keKIHVKgqU"
      },
      "source": [
        "### Model 0 — Constant Baseline (always predicts the same label)\n",
        "\n",
        "**Results (train / test):**  \n",
        "- **Accuracy:** 0.50 / 0.50  \n",
        "- **F1:** 0.00 / 0.00  \n",
        "- **ROC AUC:** 0.50 / 0.50  \n",
        "- **Average Precision (APS):** 0.50 / 0.50\n",
        "\n",
        "**What this means:**  \n",
        "- The data is roughly balanced, so always guessing one label lands at ~50% accuracy by chance.  \n",
        "- Because the model never predicts the positive class, recall for positives is 0 → **F1 is 0**.  \n",
        "- **ROC AUC** and **APS** at 0.50 indicate no real skill—basically coin-flip performance.\n",
        "\n",
        "**Takeaway & next step:**  \n",
        "This baseline just proves the pipeline runs. Now I'll switch to real features (**TF-IDF with unigrams + bigrams**) and a proper classifier (**Logistic Regression**) to push **F1 ≥ 0.85**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFj-V5GNKgqU"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Excellent work defining a clear **baseline model** with `DummyClassifier`, evaluating it with multiple metrics, and writing a concise conclusion. The commentary connects the numbers (accuracy, F1, ROC AUC, APS) to their meaning, and you’ve explained why the baseline performs as it does. The clear “takeaway & next step” transition sets up the move to real models perfectly.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhkXvpoRKgqU"
      },
      "source": [
        "### Model 1 - NLTK, TF-IDF and LR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYbjlzgaKgqU"
      },
      "source": [
        "\n",
        "Plan: tokenize → preserve negation (turn “n’t” → **“not”**) → remove most stopwords (but **keep** not/no/nor) → lemmatize → TF-IDF (uni+bi-grams) → Logistic Regression. I’ll fit the vectorizer on **train only**, transform **test**, and evaluate with the same `evaluate_model()` helper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWEC6mLmKgqU"
      },
      "outputs": [],
      "source": [
        "import nltk, re, html\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlCOxKipKgqU"
      },
      "outputs": [],
      "source": [
        "# 0) Natural Language resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# 1) review_norm, Light/Safe normalization\n",
        "if 'review_norm' not in df_reviews.columns:\n",
        "    df_reviews['review_norm'] = (df_reviews['review']\n",
        "        .str.lower()\n",
        "        .replace(r'<br\\s*/?>', ' ', regex=True)\n",
        "        .str.replace(r'<[^>]+>', ' ', regex=True)\n",
        "        .str.replace(r\"[^a-z\\s']\", ' ', regex=True)\n",
        "        .str.replace(r'\\s+', ' ', regex=True)\n",
        "        .str.strip()\n",
        "    )\n",
        "\n",
        "# 2) Train/Test splits recreation\n",
        "df_reviews_train = df_reviews.query('ds_part == \"train\"').copy()\n",
        "df_reviews_test = df_reviews.query('ds_part == \"test\"').copy()\n",
        "train_target = df_reviews_train['pos'].astype(int)\n",
        "test_target = df_reviews_test['pos'].astype(int)\n",
        "\n",
        "# 3) NLTK preprocessing(negation)\n",
        "negation_keep = {\"not\", \"no\", \"nor\", \"n't\"}\n",
        "stop_words = set(stopwords.words('english')) - negation_keep\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def text_preprocessing_1(text:str) -> str:\n",
        "    t = str(text)\n",
        "    toks = nltk.word_tokenize(t)\n",
        "    cleaned = []\n",
        "    for tok in toks:\n",
        "        tok = tok.lower()\n",
        "        if tok == \"n't\":\n",
        "            tok = \"not\"\n",
        "        if not re.search(r\"[a-z]\", tok):\n",
        "            continue\n",
        "        if tok in stop_words:\n",
        "            continue\n",
        "        tok = lemmatizer.lemmatize(tok)\n",
        "        cleaned.append(tok)\n",
        "    return \" \".join(cleaned)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# 4) 'review_nltk' columns\n",
        "\n",
        "for p in [\"punkt\", \"stopwords\", \"wordnet\", \"omw-1.4\", \"punkt_tab\"]:\n",
        "    nltk.download(p)\n",
        "\n",
        "if \"review_norm\" in df_reviews_train.columns and \"review_norm\" in df_reviews_test.columns:\n",
        "    df_reviews_train[\"review_nltk\"] = df_reviews_train[\"review_norm\"].progress_apply(text_preprocessing_1)\n",
        "    df_reviews_test[\"review_nltk\"] = df_reviews_test[\"review_norm\"].progress_apply(text_preprocessing_1)\n",
        "else:\n",
        "    print(\"Skipping: 'review_norm' not found in train/test dataframes.\")"
      ],
      "metadata": {
        "id": "iwo9ttKAJPzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 5) TF-IDF Vectorization for Model 1\n",
        "# ✅ Make sure targets exist (pos = 1 positive, 0 negative)\n",
        "train_target = df_reviews_train[\"pos\"]\n",
        "test_target = df_reviews_test[\"pos\"]\n",
        "\n",
        "# ✅ TF-IDF Vectorization (sample-friendly)\n",
        "# If your sample is small, min_df=3 can cause \"empty vocabulary\" or missing terms.\n",
        "min_df_value = 1 if len(df_reviews_train) < 50 else 3\n",
        "\n",
        "tfidf_vectorizer_1 = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=min_df_value,\n",
        "    max_df=0.9,\n",
        "    sublinear_tf=True,\n",
        "    strip_accents=\"unicode\"\n",
        ")\n",
        "\n",
        "train_features_1 = tfidf_vectorizer_1.fit_transform(df_reviews_train[\"review_nltk\"])\n",
        "test_features_1 = tfidf_vectorizer_1.transform(df_reviews_test[\"review_nltk\"])\n",
        "\n",
        "# ✅ Logistic Regression Model\n",
        "model_1 = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model_1.fit(train_features_1, train_target)\n"
      ],
      "metadata": {
        "id": "ok850HOJKJBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH5Hz6ikKgqV"
      },
      "outputs": [],
      "source": [
        "evaluate_model(model_1, train_features_1, train_target, test_features_1, test_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNu8dSYUKgqV"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Excellent work! You handled negations smartly (`n't` → `not`), preserved key stopwords, applied lemmatization, and clearly documented your preprocessing function. Training TF-IDF on train only and transforming test shows correct awareness of data leakage. Logistic Regression is a strong baseline for text classification, and your evaluation integrates seamlessly with `evaluate_model()`.\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muC5gtrOKgqV"
      },
      "source": [
        "### Model 3 — spaCy + TF-IDF + Logistic Regression\n",
        "\n",
        "**Results (train / test):**  \n",
        "- **Accuracy:** 0.95 / 0.89  \n",
        "- **F1:** 0.95 / 0.89  \n",
        "- **Average Precision (APS):** 0.99 / 0.96  \n",
        "- **ROC AUC:** 0.99 / 0.96  \n",
        "\n",
        "**summary:**  \n",
        "- This model clearly **meets the goal** (test **F1 = 0.89 ≥ 0.85**).  \n",
        "- Very high train scores and a modest drop on test suggest a **good fit with mild overfitting**, which is normal for text models.  \n",
        "- Strong **ROC AUC** and **APS** on test show it ranks positive reviews very well and maintains solid precision-recall balance.\n",
        "\n",
        "**Note:**  \n",
        "- Performance is **very close** to the NLTK version; if I need the last bit of F1, I can try small tweaks (e.g., tuning `C`, adjusting `min_df`, or experimenting with stopword handling), but this is already a strong result.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcRVtIk6KgqV"
      },
      "source": [
        "### Model 3 - spaCy, TF-IDF and LR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBfl5X14KgqV"
      },
      "source": [
        "I’ll lemmatize text with spaCy, keep negations (e.g., “not”), vectorize with TF-IDF (uni+bi-grams), and train Logistic Regression. The vectorizer is fit on **train only** and evaluated on **test** with `evaluate_model()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syuBLpW9KgqV"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 1) spaCy\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRkAJtazKgqV"
      },
      "outputs": [],
      "source": [
        "# 2) spaCy preprocessing (light/clean text)\n",
        "def text_preprocessing_3(text: str) -> str:\n",
        "\n",
        "    doc = nlp(str(text))\n",
        "    lemmas = [tok.lemma_.lower() for tok in doc if not tok.is_space]\n",
        "    return \" \".join(lemmas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIPHDRfXKgqV"
      },
      "outputs": [],
      "source": [
        "# 3) Preprocessed columns\n",
        "df_reviews_train[\"review_spacy\"] = df_reviews_train[\"review_norm\"].progress_apply(text_preprocessing_3)\n",
        "df_reviews_test[\"review_spacy\"] = df_reviews_test[\"review_norm\"].progress_apply(text_preprocessing_3)\n",
        "\n",
        "# 4) Vectorized(Train/Text)\n",
        "tfidf_3 = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=3,\n",
        "    max_df=0.9,\n",
        "    sublinear_tf=True,\n",
        "    strip_accents=\"unicode\"\n",
        ")\n",
        "train_features_3 = tfidf_3.fit_transform(df_reviews_train[\"review_spacy\"])\n",
        "test_features_3 = tfidf_3.transform(df_reviews_test[\"review_spacy\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHw4kaqqKgqV"
      },
      "outputs": [],
      "source": [
        "# 5) Logistic Regression\n",
        "model_3 = LogisticRegression(\n",
        "    C=4.0,\n",
        "    solver=\"liblinear\",\n",
        "    max_iter=2000,\n",
        "    random_state=42\n",
        ")\n",
        "model_3.fit(train_features_3, train_target.astype(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nj074feKgqV"
      },
      "outputs": [],
      "source": [
        "# 6) Evaluation\n",
        "evaluate_model(\n",
        "    model_3,\n",
        "    train_features_3, train_target.astype(int),\n",
        "    test_features_3, test_target.astype(int)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKj9D-G5KgqV"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Excellent job building the spaCy preprocessing pipeline and integrating it with TF-IDF and Logistic Regression. The evaluation shows a strong model with test F1 = 0.89 (above the 0.85 target), and you clearly documented your methodology, metrics, and interpretation. The plots (F1 vs threshold, ROC, PRC) add valuable depth to your analysis and support your conclusions.\n",
        "</div>\n",
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Suggestion: the results show very high training performance (F1 ≈ 0.95, ROC AUC = 0.99) compared to slightly lower test performance (F1 ≈ 0.89, ROC AUC = 0.96). This indicates mild <b>overfitting</b>, which is common in text models. You might mention or test regularization adjustments (e.g., lowering `C`) or slight changes to `min_df` to see if they reduce the gap. Not critical here, since the model already meets the project goal.\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzdMSfQoKgqV"
      },
      "source": [
        "### Model 4 - spaCy, TF-IDF and LGBMClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2KGJNJIKgqV"
      },
      "source": [
        "I’m keeping the same text representation as Model 3 (spaCy lemmas → TF-IDF with uni+bi-grams), but switching the classifier to **LightGBM**. Gradient boosting can sometimes grab non-linear interactions that linear models miss.\n",
        "\n",
        "**Plan**\n",
        "- Use the `review_spacy` text (lemmatized with spaCy).\n",
        "- Vectorize with TF-IDF (fit on train only; transform test).\n",
        "- Train `LGBMClassifier` and evaluate with the same helper (`evaluate_model`), which uses `predict_proba`.\n",
        "\n",
        "**Notes**\n",
        "- LightGBM handles sparse TF-IDF directly.\n",
        "- If LightGBM isn’t installed locally, I’ll see an import error and can install it with: `pip install lightgbm`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laNP00a4KgqW"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP-9dForKgqW"
      },
      "outputs": [],
      "source": [
        "# 1) TF-IDF on spaCy lemmas (TRAIN)\n",
        "tfidf_4 = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=3,\n",
        "    max_df=0.9,\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode'\n",
        ")\n",
        "train_features_4 = tfidf_4.fit_transform(df_reviews_train['review_spacy'])\n",
        "test_features_4  = tfidf_4.transform(df_reviews_test['review_spacy'])\n",
        "\n",
        "# 2) LightGBM\n",
        "model_4 = LGBMClassifier(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.1,\n",
        "    num_leaves=31,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yskmYuGgKgqW"
      },
      "outputs": [],
      "source": [
        "# 4) Trained\n",
        "model_4.fit(train_features_4, train_target.astype(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n3xVCaSKgqW"
      },
      "outputs": [],
      "source": [
        "# 5) Evaluation\n",
        "evaluate_model(\n",
        "    model_4,\n",
        "    train_features_4, train_target.astype(int),\n",
        "    test_features_4, test_target.astype(int)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z85E7wYnKgqW"
      },
      "source": [
        "### Model 4 — spaCy + TF-IDF + LightGBM\n",
        "\n",
        "**Results (train / test):**  \n",
        "- **Accuracy:** 0.99 / 0.89  \n",
        "- **F1:** 0.99 / 0.89  \n",
        "- **Average Precision (APS):** 1.00 / 0.95  \n",
        "- **ROC AUC:** 1.00 / 0.96  \n",
        "\n",
        "**Plain-English takeaways:**  \n",
        "- The model **meets the target** with **F1 = 0.89** on the test set.  \n",
        "- Train scores are near-perfect while test is lower → **mild overfitting**, which is common on high-dimensional sparse text.  \n",
        "- Strong **ROC AUC** and **APS** show good ranking and precision–recall behavior.\n",
        "\n",
        "**Notes vs. Logistic Regression:**  \n",
        "- Performance is **very close** to the LR model; this is typical—linear models often excel on TF-IDF features.  \n",
        "- LightGBM didn’t beat LR here, but it’s still a solid alternative.\n",
        "\n",
        "**Possible tweaks:**  \n",
        "- Tune `num_leaves`, `n_estimators`, and `learning_rate`.  \n",
        "- Try dimensionality reduction (e.g., **TruncatedSVD** to 300–500 comps) before LightGBM to reduce sparsity.  \n",
        "- Consider threshold tuning to squeeze a bit more test F1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4zfboV8KgqW"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Excellent work with your LightGBM experiment! You correctly reused the spaCy + TF-IDF pipeline, applied solid hyperparameters, and clearly documented results in a markdown conclusion. The test F1 = 0.89 meets the project target, and your plain-English takeaways (balanced with notes on overfitting and LR comparison) make the results easy to understand. Highlighting possible tweaks (regularization, dimensionality reduction, threshold tuning) shows good forward thinking.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52_rzsLPKgqW"
      },
      "source": [
        "###  Model 9 - BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8kTFM0CKgqW"
      },
      "source": [
        "I’ll convert texts into **BERT [CLS] embeddings** using `bert-base-uncased`, then train a simple **Logistic Regression** on those embeddings. Because full IMDB on CPU can take hours, I’m running a **small sample (≈200 train / 200 test)** to compare against the TF-IDF models. If a GPU is available, the code will use it automatically; otherwise it runs on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M5SL9g9KgqW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBEb9E2FKgqW"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "config = transformers.BertConfig.from_pretrained('bert-base-uncased')\n",
        "model = transformers.BertModel.from_pretrained('bert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhXR-xgPKgqW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def BERT_text_to_embeddings(texts, max_length=512, batch_size=100, force_device=None, disable_progress_bar=False):\n",
        "\n",
        "    ids_list = []\n",
        "    attention_mask_list = []\n",
        "\n",
        "    # 1) Tokenization\n",
        "    enc = tokenizer(\n",
        "        list(texts),\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "    ids_list = enc['input_ids']\n",
        "    attention_mask_list = enc['attention_mask']\n",
        "\n",
        "    # 2) Device\n",
        "    if force_device is not None:\n",
        "        device = torch.device(force_device)\n",
        "    else:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model.to(device)\n",
        "    if not disable_progress_bar:\n",
        "        print(f'Using the {device} device.')\n",
        "\n",
        "    # 3) gettings embeddings in batches\n",
        "    embeddings = []\n",
        "    n_batches = math.ceil(len(ids_list) / batch_size)\n",
        "\n",
        "    for i in tqdm(range(math.ceil(len(ids_list)/batch_size)), disable=disable_progress_bar):\n",
        "        start = batch_size * i\n",
        "        end = batch_size * (i + 1)\n",
        "\n",
        "        ids_batch = torch.LongTensor(ids_list[batch_size*i:batch_size*(i+1)]).to(device)\n",
        "        attention_mask_batch = torch.LongTensor(attention_mask_list[start:end]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            outputs = model(input_ids=ids_batch, attention_mask=attention_mask_batch)\n",
        "            cls_batch = outputs.last_hidden_state[:, 0, :]\n",
        "            embeddings.append(cls_batch.detach().cpu().numpy())\n",
        "\n",
        "    return np.concatenate(embeddings, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-Qe9KslKgqW"
      },
      "outputs": [],
      "source": [
        "# 1) Sample ~ 200 from each split\n",
        "n_train = min(200, len(df_reviews_train))\n",
        "n_test = min(200, len(df_reviews_test))\n",
        "\n",
        "train_sample = df_reviews_train.sample(n=n_train, random_state=42)\n",
        "test_sample = df_reviews_test.sample(n=n_test, random_state=42)\n",
        "\n",
        "features_train_bert = train_sample['review_norm']\n",
        "target_train_bert = train_sample['pos'].astype(int).values\n",
        "\n",
        "features_test_bert = test_sample['review_norm']\n",
        "target_test_bert = test_sample['pos'].astype(int).values\n",
        "\n",
        "# 2) Embeddings\n",
        "train_features_9 = BERT_text_to_embeddings(features_train_bert, max_length=256, batch_size=32)\n",
        "test_features_9 = BERT_text_to_embeddings(features_test_bert, max_length=256, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MZ52LDSKgqW"
      },
      "outputs": [],
      "source": [
        "print(\"Embeddings shape (train/test):\", train_features_9.shape, test_features_9.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBtCwygZKgqW"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model_9 = LogisticRegression(\n",
        "    C=2.0, max_iter=2000, solver='lbfgs', random_state=42\n",
        ")\n",
        "\n",
        "model_9.fit(train_features_9, target_train_bert)\n",
        "\n",
        "evaluate_model(\n",
        "    model_9,\n",
        "    train_features_9, target_train_bert,\n",
        "    test_features_9, target_test_bert\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erpaujfIKgqX"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Great effort experimenting with BERT! You correctly sampled a manageable subset, generated embeddings, and trained a classifier on top. Your batching logic, device management, and use of `with torch.no_grad()` are all implemented properly. It shows you understand both the heavy computational cost of transformers and how to integrate them into your pipeline.\n",
        "</div>\n",
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Suggestion: since BERT is computationally demanding and TripleTen doesn’t support GPU acceleration, it’s expected that running this code may crash or kill the kernel. For the project submission, it’s completely fine to note that you attempted this experiment but could not fully evaluate results due to hardware limits. Including just the setup and rationale (as you’ve done) is enough to demonstrate understanding.\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut1mlFYwKgqX"
      },
      "outputs": [],
      "source": [
        "# Embeddings saved to disk\n",
        "import numpy as np\n",
        "\n",
        "np.savez_compressed(\n",
        "    'features_9.npz',\n",
        "    train_features_9=train_features_9,\n",
        "    test_features_9=test_features_9\n",
        ")\n",
        "print(\"saved to features_9.npz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rL9Gb_DKgqX"
      },
      "outputs": [],
      "source": [
        "# Saved indices of the sampled rows\n",
        "np.savez_compressed(\n",
        "    'features_9_meta.npz',\n",
        "    train_index=getattr(train_sample.index, \"values\", None),\n",
        "    test_index=getattr(test_sample.index, \"values\", None)\n",
        ")\n",
        "\n",
        "train_features_9.shape[0] == len(target_train_bert)\n",
        "test_features_9.shape[0] == len(target_test_bert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etXmw-zsKgqX"
      },
      "source": [
        "### Model X — Results\n",
        "\n",
        "**Results (train / test):**  \n",
        "- **Accuracy:** 1.00 / 0.76  \n",
        "- **F1:** 1.00 / 0.74  \n",
        "- **Average Precision (APS):** 1.00 / 0.83  \n",
        "- **ROC AUC:** 1.00 / 0.85\n",
        "\n",
        "**What this tells me:**  \n",
        "- The model is **perfect on train** but drops notably on test → classic **overfitting** (or potential leakage).  \n",
        "- Despite decent ranking ability on test (ROC AUC **0.85**, APS **0.83**), the **operating F1 = 0.74** is **below the project target (≥ 0.85)** and below my stronger baselines.\n",
        "\n",
        "**Checks I should make right away:**  \n",
        "- Ensure the vectorizer/feature extractor was **fit on train only** and then **transformed** test (no leakage).  \n",
        "- Confirm train/test separation for any learned preprocessing (e.g., SVD fit, BERT embeddings, scalers).  \n",
        "- Verify I’m not accidentally mixing columns/splits when building features.\n",
        "\n",
        "**How I’ll try to fix it:**  \n",
        "- **Regularize more:**  \n",
        "  - Logistic Regression → lower `C` (e.g., 1.0 → 0.5 or 0.25).  \n",
        "  - LightGBM → reduce `num_leaves`, increase `min_child_samples`, use smaller `feature_fraction`.  \n",
        "- **Dimensionality reduction:** Try TF-IDF → **TruncatedSVD (300–500)** → classifier to reduce sparsity.  \n",
        "- **Threshold tuning:** Pick the F1-optimal threshold from my F1-vs-threshold curve instead of 0.50.  \n",
        "- **Calibrate probabilities:** `CalibratedClassifierCV` (for SVMs) or `class_weight='balanced'` if needed.  \n",
        "- **Data-side tweaks:** Preserve negations (“n’t” → “not”), keep bigrams, and add a few hard negative/irony examples.\n",
        "\n",
        "**Bottom line:** As-is, this model **doesn’t meet the target** on test. I’ll either improve it with the steps above or proceed with my better-performing model (e.g., the spaCy + TF-IDF + LR that hit ~**0.89–0.90 F1** on test) as the primary solution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tysSHhw6KgqX"
      },
      "source": [
        "## My Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO3f2qmAKgqX"
      },
      "source": [
        "### New hand-labeled reviews for out-of-sample testing\n",
        "\n",
        "Below are fresh reviews I wrote with **known ground truth** so I can test my models on text they’ve never seen.  \n",
        "- `gold = 1` → positive  \n",
        "- `gold = 0` → negative  \n",
        "\n",
        "I included tricky cases (negation, contrast, mild sarcasm, mixed sentiment) to stress-test the classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqwSk8XXKgqX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "my_reviews = pd.DataFrame([\n",
        "    {\"review\": \"I didn't expect much, but this was a delightful surprise.\", \"gold\": 1},\n",
        "    {\"review\": \"Not even the soundtrack could save this mess.\", \"gold\": 0},\n",
        "    {\"review\": \"The pacing isn't perfect, yet I left the theater smiling.\", \"gold\": 1},\n",
        "    {\"review\": \"I would not recommend this to my worst enemy.\", \"gold\": 0},\n",
        "    {\"review\": \"Smart, heartfelt, and beautifully shot.\", \"gold\": 1},\n",
        "    {\"review\": \"What a waste of two hours.\", \"gold\": 0},\n",
        "    {\"review\": \"It's not bad at all—actually pretty great.\", \"gold\": 1},\n",
        "    {\"review\": \"Jokes land flat, and the lead seems lost.\", \"gold\": 0},\n",
        "    {\"review\": \"I cried, I laughed, I want to see it again.\", \"gold\": 1},\n",
        "    {\"review\": \"The plot twists are obvious and tiresome.\", \"gold\": 0},\n",
        "    {\"review\": \"Better than the last one by a mile.\", \"gold\": 1},\n",
        "    {\"review\": \"I tried twice and still couldn't finish it.\", \"gold\": 0},\n",
        "    {\"review\": \"A cozy, feel-good story I'll rewatch.\", \"gold\": 1},\n",
        "    {\"review\": \"Meh—forgettable and bland.\", \"gold\": 0},\n",
        "    {\"review\": \"Uneven first half, strong finish; overall I liked it.\", \"gold\": 1},\n",
        "    {\"review\": \"Technically impressive, emotionally empty.\", \"gold\": 0},\n",
        "    {\"review\": \"Not only entertaining but also thoughtful.\", \"gold\": 1},\n",
        "    {\"review\": \"I kept checking the time—never a good sign.\", \"gold\": 0},\n",
        "    {\"review\": \"Yeah, totally 'hilarious'—I didn't laugh once.\", \"gold\": 0},  # sarcasm\n",
        "    {\"review\": \"This is the kind of movie that stays with you.\", \"gold\": 1},\n",
        "    {\"review\": \"I regret renting this.\", \"gold\": 0},\n",
        "], columns=[\"review\", \"gold\"])\n",
        "\n",
        "# THE SAME normalization I used for the main dataset\n",
        "my_reviews[\"review_norm\"] = (\n",
        "    my_reviews[\"review\"]\n",
        "      .str.lower()\n",
        "      .str.replace(r\"<br\\s*/?>\", \" \", regex=True)\n",
        "      .str.replace(r\"<[^>]+>\", \" \", regex=True)\n",
        "      .str.replace(r\"[^a-z\\s']\", \" \", regex=True)  # keep apostrophes\n",
        "      .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "      .str.strip()\n",
        ")\n",
        "\n",
        "def print_preds(name, probs, texts, gold, threshold=0.5, preview_chars=100):\n",
        "    preds = (probs >= threshold).astype(int)\n",
        "    acc = (preds == gold).mean()\n",
        "    print(f\"\\n{name} - mini-set accuracy @ {threshold:.2f}: {acc:.2%}\")\n",
        "    for i, review in enumerate(texts.str.slice(0, preview_chars)):\n",
        "        print(f\"{probs[i]:.2f}:  {review}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5-BZwXBKgqX"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Great initiative creating your own small labeled dataset for validation! Normalizing these reviews with the same pipeline ensures consistency, and the `print_preds` helper is a neat way to quickly inspect model probabilities, gold labels, and predictions. This shows strong practical awareness of testing models on unseen, real-like data.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPfEel_MKgqX"
      },
      "source": [
        "### Model 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUfz2BXmKgqX"
      },
      "source": [
        "### Model 1 — Predictions on my hand-labeled reviews\n",
        "\n",
        "I transform the normalized texts with the same TF-IDF used to train Model 1, get the predicted **probability of positive**, print the first 100 chars of each review with the **predicted label**, and—if `gold` is present—compute quick accuracy/F1 on this mini set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjjDq3bvKgqX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "texts = my_reviews['review_norm']\n",
        "\n",
        "X_m1 = tfidf_vectorizer_1.transform(texts)\n",
        "my_reviews_pred_prob = model_1.predict_proba(X_m1)[:, 1]\n",
        "\n",
        "threshold = 0.45\n",
        "pred_labels = (my_reviews_pred_prob >= threshold).astype(int)\n",
        "\n",
        "\n",
        "print(\"Model 1 — per-review predictions (p=probability of positive, pred=label)\")\n",
        "for i, preview in enumerate(my_reviews['review'].str.slice(0, 100)):\n",
        "    print(f\"p={my_reviews_pred_prob[i]:.2f}  pred={pred_labels[i]}  |  {preview}\")\n",
        "\n",
        "# Metrics on mini set\n",
        "if 'gold' in my_reviews.columns:\n",
        "    y_true = my_reviews['gold'].to_numpy()\n",
        "    acc = accuracy_score(y_true, pred_labels)\n",
        "    f1  = f1_score(y_true, pred_labels)\n",
        "    prec = precision_score(y_true, pred_labels, zero_division=0)\n",
        "    rec  = recall_score(y_true, pred_labels,    zero_division=0)\n",
        "    print(f\"\\nMini-set metrics @ threshold {threshold:.2f}: \"\n",
        "          f\"Accuracy={acc:.2%} | F1={f1:.2%} | Precision={prec:.2%} | Recall={rec:.2%}\")\n",
        "    print(\"Confusion matrix (rows=true, cols=pred):\\n\", confusion_matrix(y_true, pred_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UGH7Ig5KgqX"
      },
      "source": [
        "### Conclusions on my “real” reviews — Model 1 (TF-IDF + LR)\n",
        "\n",
        "**Mini-set results @ threshold 0.50**  \n",
        "- Accuracy: **66.7%**, F1: **66.7%** (Precision **63.6%**, Recall **70.0%**)  \n",
        "- Confusion matrix (rows=true, cols=pred):  \n",
        "  `[[TN=7, FP=4], [FN=3, TP=7]]` → **11 negatives**, **10 positives**\n",
        "\n",
        "**What went well**  \n",
        "- Clear, strongly worded opinions were classified correctly (e.g., *“delightful surprise”*, *“waste of two hours”*, *“heartfelt and beautifully shot”*).  \n",
        "- The model handled many negations, getting *“I would not recommend this…”* and *“Not even the soundtrack could save this mess.”* right.\n",
        "\n",
        "**Where it stumbled (examples)**  \n",
        "- **Negation/contrast with praise** → *“It’s not bad at all—actually pretty great.”* predicted **negative** (FN).  \n",
        "- **Subtle/ironic tone** → *“Yeah, totally ‘hilarious’—I didn’t laugh once.”* predicted **positive** (FP).  \n",
        "- **Mixed/nuanced phrasing** → *“Technically impressive, emotionally empty.”* predicted **positive** (FP).  \n",
        "- **Borderline probabilities near 0.5** → *“Better than the last one by a mile.”* (0.48) and  \n",
        "  *“This is the kind of movie that stays with you.”* (0.50) flipped to the wrong side due to threshold.\n",
        "\n",
        "**Takeaways**  \n",
        "- On this small, deliberately tricky set, performance drops compared to the main test set (where F1 ≈ 0.90).  \n",
        "- Most errors are **edge cases**: sarcasm, nuanced praise/critique, and borderline confidences. That’s typical for bag-of-ngrams models.\n",
        "\n",
        "**Easy improvements to try**  \n",
        "- **Tune the decision threshold** using the F1-vs-threshold curve (e.g., try 0.45 on this mini set to reduce false negatives like *“not bad … pretty great”*).  \n",
        "- Ensure tokenization preserves negation cues (keep apostrophes; treat “n’t” → “not”)—already done, but verify in the vectorizer.  \n",
        "- Add a few **hard examples** (sarcasm/contrast) to the training set or use data augmentation.  \n",
        "\n",
        "**Bottom line:** The model is strong on clear sentiment but still trips on **sarcasm and nuanced wording**. With threshold tuning and a bit of targeted data, I can likely bump this mini-set F1 noticeably.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4pZFWsAKgqX"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Excellent work testing your model on a hand-crafted mini-dataset! You not only ran metrics (accuracy, F1, precision, recall, confusion matrix), but also analyzed specific false positives/negatives and explained why the model struggled (sarcasm, nuanced tone, borderline probabilities). The structured conclusion (what went well, where it stumbled, takeaways, easy improvements) is clear and professional.\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01TSTiE9KgqY"
      },
      "source": [
        "### Model 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5cFPJb3KgqY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Existing vectorizer\n",
        "try:\n",
        "    tfidf_vectorizer_3\n",
        "except NameError:\n",
        "    tfidf_vectorizer_3 = tfidf_3  # alias if you named it tfidf_3 before\n",
        "\n",
        "# Preprocessed spaCy function\n",
        "texts = my_reviews['review_norm']\n",
        "texts_m3 = texts.apply(text_preprocessing_3)\n",
        "\n",
        "X_m3 = tfidf_vectorizer_3.transform(texts_m3)\n",
        "my_reviews_pred_prob_m3 = model_3.predict_proba(X_m3)[:, 1]\n",
        "\n",
        "threshold = 0.5\n",
        "pred_labels_m3 = (my_reviews_pred_prob_m3 >= threshold).astype(int)\n",
        "\n",
        "print(\"Model 3 — per-review predictions (p=probability of positive, pred=label)\")\n",
        "for i, preview in enumerate(my_reviews['review'].str.slice(0, 100)):\n",
        "    print(f\"p={my_reviews_pred_prob_m3[i]:.2f}  pred={pred_labels_m3[i]}  |  {preview}\")\n",
        "\n",
        "if 'gold' in my_reviews.columns:\n",
        "    y_true = my_reviews['gold'].to_numpy()\n",
        "    acc = accuracy_score(y_true, pred_labels_m3)\n",
        "    f1  = f1_score(y_true, pred_labels_m3)\n",
        "    prec = precision_score(y_true, pred_labels_m3, zero_division=0)\n",
        "    rec  = recall_score(y_true, pred_labels_m3,    zero_division=0)\n",
        "    print(f\"\\nModel 3 mini-set @ {threshold:.2f}: \"\n",
        "          f\"Accuracy={acc:.2%} | F1={f1:.2%} | Precision={prec:.2%} | Recall={rec:.2%}\")\n",
        "    print(\"Confusion matrix (rows=true, cols=pred):\\n\", confusion_matrix(y_true, pred_labels_m3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEoGCPlDKgqY"
      },
      "source": [
        "### Conclusions on my “real” reviews — Model 3 (spaCy + TF-IDF + LR) @ threshold = 0.50\n",
        "\n",
        "**Mini-set results:**  \n",
        "- **Accuracy:** 90.48%  **F1:** 90.00%  (**Precision:** 90.00%, **Recall:** 90.00%)  \n",
        "- Confusion matrix (rows=true, cols=pred): `[[TN=10, FP=1], [FN=1, TP=9]]` → **11 negatives**, **10 positives**\n",
        "\n",
        "**Highlights**\n",
        "- Strong, balanced performance: only **one false positive** and **one false negative**.\n",
        "- Borderline but correct: *“Technically impressive, emotionally empty.”* → **neg** with **p=0.49** (right on the edge).\n",
        "- Handles sarcasm better than Model 1: *“Yeah, totally 'hilarious'—I didn’t laugh once.”* → **neg** (p=0.41).\n",
        "- Confident on clear positives: *“Not only entertaining but also thoughtful.”* (p=0.75),  \n",
        "  *“This is the kind of movie that stays with you.”* (p=0.60).\n",
        "\n",
        "**Remaining miss**\n",
        "- *“I kept checking the time—never a good sign.”* predicted **positive** (p=0.82) → a **false positive**; the phrasing is subtle/idiomatic.\n",
        "\n",
        "**Takeaways**\n",
        "- spaCy lemmatization + TF-IDF shows **better robustness** to nuance than Model 1 on this mini set.\n",
        "- With only one FP/FN, the current **0.50 threshold** is already well-calibrated. If I wanted to be stricter on negatives, I could raise the threshold slightly to reduce FPs (at the cost of some recall).\n",
        "\n",
        "**Bottom line:** Model 3 generalizes well to my out-of-sample reviews and comfortably meets the project target on this mini evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnV4t60uKgqY"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Fantastic job running your spaCy + TF-IDF + Logistic Regression model on the custom mini dataset. The metrics (Accuracy 90%, F1 90%) are very strong, and your structured commentary (highlights, remaining miss, takeaways, bottom line) shows deep understanding. I especially like how you tied specific predictions (sarcasm, borderline cases, idioms) back to why the model succeeded or failed. This is exemplary error analysis.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE7hMs44KgqY"
      },
      "source": [
        "### Model 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLSLm8oBKgqY"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "tfidf_vectorizer_4 = tfidf_vectorizer_3\n",
        "\n",
        "texts = my_reviews['review_norm']\n",
        "texts_m4 = texts.apply(text_preprocessing_3)\n",
        "\n",
        "X_m4 = tfidf_vectorizer_4.transform(texts_m4)\n",
        "my_reviews_pred_prob_m4 = model_4.predict_proba(X_m4)[:, 1]\n",
        "\n",
        "threshold = 0.5\n",
        "pred_labels_m4 = (my_reviews_pred_prob_m4 >= threshold).astype(int)\n",
        "\n",
        "print(\"Model 4 — per-review predictions (p=probability of positive, pred=label)\")\n",
        "for i, preview in enumerate(my_reviews['review'].str.slice(0, 100)):\n",
        "    print(f\"p={my_reviews_pred_prob_m4[i]:.2f}  pred={pred_labels_m4[i]}  |  {preview}\")\n",
        "\n",
        "if 'gold' in my_reviews.columns:\n",
        "    y_true = my_reviews['gold'].to_numpy()\n",
        "    acc = accuracy_score(y_true, pred_labels_m4)\n",
        "    f1  = f1_score(y_true, pred_labels_m4)\n",
        "    prec = precision_score(y_true, pred_labels_m4, zero_division=0)\n",
        "    rec  = recall_score(y_true, pred_labels_m4,    zero_division=0)\n",
        "    print(f\"\\nModel 4 mini-set @ {threshold:.2f}: \"\n",
        "          f\"Accuracy={acc:.2%} | F1={f1:.2%} | Precision={prec:.2%} | Recall={rec:.2%}\")\n",
        "    print(\"Confusion matrix (rows=true, cols=pred):\\n\", confusion_matrix(y_true, pred_labels_m4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBOb2VfgKgqY"
      },
      "source": [
        "### Conclusions on my “real” reviews — Model 4 (spaCy + TF-IDF + LightGBM) @ threshold = 0.50\n",
        "\n",
        "**Mini-set results:**  \n",
        "- **Accuracy:** 71.43%  **F1:** 76.92% (**Precision:** 62.50%, **Recall:** 100.00%)  \n",
        "- Confusion matrix (rows=true, cols=pred): `[[TN=5, FP=6], [FN=0, TP=10]]` → **11 negatives**, **10 positives**\n",
        "\n",
        "**What this says about the model**\n",
        "- It’s tuned toward **catching positives** (Recall = **100%**); it didn’t miss any positive review on this mini set.  \n",
        "- The trade-off is **low precision** (many **false positives**). Six negatives were labeled positive.\n",
        "\n",
        "**Typical errors (false positives)**\n",
        "- **Subtle/ironic negatives** marked positive:  \n",
        "  *“Yeah, totally ‘hilarious’—I didn’t laugh once.”* (**p=0.77**)  \n",
        "  *“I kept checking the time—never a good sign.”* (**p=0.86**)  \n",
        "  *“Technically impressive, emotionally empty.”* (**p=0.72**)  \n",
        "  *“I regret renting this.”* (**p=0.66**)  \n",
        "- Likely cause: boosting on sparse n-grams can over-weight isolated positive cue words (*impressive*, *hilarious*, *kind of movie*, etc.) and miss the **overall negative composition** or sarcasm.\n",
        "\n",
        "**How to improve**\n",
        "- **Raise the decision threshold** (e.g., 0.60–0.65) to trade a bit of recall for better precision on negatives. Pick it from the **PR/F1 curves**.  \n",
        "- Add more **sarcasm/contrast** examples to training (or reweight such cases).  \n",
        "- Try **regularizing LightGBM** more (`num_leaves↓`, `min_child_samples↑`, `feature_fraction/colsample_bytree↓`).  \n",
        "- Consider **TF-IDF → TruncatedSVD (300–500 dims) → LightGBM** to reduce sparsity and soften spurious n-grams.  \n",
        "- In this project, **Model 3 (LR)** handled nuance better on the same mini set, so it remains the safer default.\n",
        "\n",
        "**Bottom line:** Model 4 is **recall-heavy** and meets the goal on the main test set, but on these hand-written reviews it flags too many negatives as positive. A higher threshold and some regularization should close the gap.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LojO84mMKgqY"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "  <b>Reviewer’s comment – Iteration 1:</b><br>\n",
        "  Excellent follow-up evaluation of your LightGBM model on the custom mini dataset. You clearly documented results (Acc ≈ 71%, F1 ≈ 77%, Recall = 100%) and explained the trade-off between recall and precision. The error analysis with concrete false positive examples (sarcasm, subtle negatives) is very strong and shows a nuanced understanding of how boosting can overweight isolated cue words in sparse text.\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--86VeJxKgqY"
      },
      "source": [
        "### Model 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqtWckQeKgqY"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    f1_score, average_precision_score, roc_auc_score,\n",
        "    precision_recall_curve, accuracy_score, precision_score, recall_score, confusion_matrix\n",
        ")\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# 1) Validation split from TRAIN (to tune threshold without touching test)\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "(tr_idx, val_idx) = next(sss.split(train_features_9, target_train_bert))\n",
        "feat_tr9, target_tr9   = train_features_9[tr_idx], target_train_bert[tr_idx]\n",
        "feat_val9, target_val9 = train_features_9[val_idx], target_train_bert[val_idx]\n",
        "\n",
        "# model_9 refit on the training portion\n",
        "model_9.fit(feat_tr9, target_tr9)\n",
        "\n",
        "# 2) Threshold for the NEGATIVE class\n",
        "#    negatives (label==0) as the \"positive\" class for metric purposes\n",
        "val_p_pos = model_9.predict_proba(feat_val9)[:, 1]     # P(positive)\n",
        "val_p_neg = 1.0 - val_p_pos                         # P(negative)\n",
        "\n",
        "ts = np.linspace(0, 1, 401)\n",
        "f1s = [f1_score((target_val9==0).astype(int), (val_p_neg >= t).astype(int)) for t in ts]\n",
        "t_star = ts[int(np.argmax(f1s))]\n",
        "print(f\"Best threshold for NEGATIVE on validation: {t_star:.2f} (F1={max(f1s):.3f})\")\n",
        "\n",
        "# 3) Final evaluation on TEST at tuned threshold (NEGATIVE focus)\n",
        "test_p_pos = model_9.predict_proba(test_features_9)[:, 1]\n",
        "test_p_neg = 1.0 - test_p_pos\n",
        "\n",
        "target_true_neg = (target_test_bert==0).astype(int)\n",
        "target_pred_neg = (test_p_neg >= t_star).astype(int)\n",
        "\n",
        "acc  = accuracy_score(target_true_neg, target_pred_neg)\n",
        "f1   = f1_score(target_true_neg, target_pred_neg)\n",
        "prec = precision_score(target_true_neg, target_pred_neg, zero_division=0)\n",
        "rec  = recall_score(target_true_neg, target_pred_neg,    zero_division=0)\n",
        "aps  = average_precision_score(target_true_neg, test_p_neg)  # PR AUC for negative detection\n",
        "roc  = roc_auc_score(target_true_neg, test_p_neg)\n",
        "cm   = confusion_matrix(target_true_neg, target_pred_neg)\n",
        "\n",
        "print(f\"\\nModel 9 — NEGATIVE detection @ threshold {t_star:.2f}\")\n",
        "print(f\"Accuracy={acc:.3f} | F1={f1:.3f} | Precision={prec:.3f} | Recall={rec:.3f}\")\n",
        "print(f\"APS (PR AUC)={aps:.3f} | ROC AUC={roc:.3f}\")\n",
        "print(\"Confusion matrix (rows=true_neg, cols=pred_neg):\\n\", cm)\n",
        "\n",
        "# 4) POSITIVE-class metrics for completeness\n",
        "target_true_pos = (target_test_bert==1).astype(int)\n",
        "target_pred_pos = (test_p_pos >= (1 - t_star)).astype(int)  # complementary threshold\n",
        "print(\"\\n(Also for POSITIVE class)\")\n",
        "print(\"F1_pos=\", f1_score(target_true_pos, target_pred_pos))\n",
        "print(\"APS_pos=\", average_precision_score(target_true_pos, test_p_pos), \" ROC_pos=\", roc_auc_score(target_true_pos, test_p_pos))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B59BsbGZKgqY"
      },
      "source": [
        "\n",
        "### Model 9 (BERT embeddings + LR) — Focusing on **both** classes\n",
        "\n",
        "**Validation tuning:** I optimized the decision threshold to better detect **negative** reviews, which gave  \n",
        "**t\\* = 0.31** on the *negative* score (i.e., predict negative if *p(neg) ≥ 0.31*).\n",
        "\n",
        "**Test results at t\\* = 0.31**\n",
        "\n",
        "- **Negative class (our tuned target)**\n",
        "  - **F1_neg:** **0.798**  | **Precision_neg:** 0.744  | **Recall_neg:** **0.861**\n",
        "  - **APS_neg (PR AUC):** 0.827  | **ROC AUC_neg:** 0.817\n",
        "\n",
        "- **Positive class** *(complementary threshold; p(pos) ≥ 0.69)*  \n",
        "  - **F1_pos:** **0.719**  | **APS_pos:** 0.811  | **ROC AUC_pos:** 0.817\n",
        "\n",
        "- **Overall**\n",
        "  - **Accuracy:** 0.765  \n",
        "  - **Macro-F1 (avg of pos/neg F1):** ≈ **0.759**  \n",
        "  - **Confusion matrix** *(rows = true_neg, cols = pred_neg)*:\n",
        "    ```\n",
        "    [[60 32]\n",
        "     [15 93]]\n",
        "    ```\n",
        "    → 92 positive reviews (row 0), 108 negative reviews (row 1).  \n",
        "      Correct: 60 true positives (for the positive class) + 93 true negatives (for the negative class).  \n",
        "      Errors: 32 positives flagged as negative (FP_neg) and 15 negatives missed (FN_neg).\n",
        "\n",
        "**Interpretation (both classes)**\n",
        "- With t\\* = 0.31 the model is **recall-oriented for negatives** (it catches most negative reviews), which is often desirable if the goal is to surface harmful content or dissatisfied users.  \n",
        "- The **positive class** remains solid (F1 ≈ 0.72), so performance is reasonably balanced across both sides.\n",
        "- The ranking quality is consistent for both classes (**APS/ROC ≈ 0.81–0.83**), meaning scores are well ordered even before thresholding.\n",
        "\n",
        "**If I want a more symmetric balance**\n",
        "- I can re-tune the threshold to **maximize Macro-F1** (averaging F1 for positive and negative) on a validation split.  \n",
        "- Alternatively, use **dual thresholds** for triage:\n",
        "  - `p(pos) ≥ 0.7` → confidently **positive**  \n",
        "  - `p(pos) ≤ 0.3` → confidently **negative**  \n",
        "  - `0.3 < p(pos) < 0.7` → **manual review** / second pass  \n",
        "  This reduces mistakes at the cost of leaving some reviews unclassified automatically.\n",
        "\n",
        "**Conclusion**  \n",
        "Model 9 performs well for **both positive and negative** detection. At the negative-optimized threshold it delivers high **F1_neg (0.80)** while keeping **F1_pos (~0.72)** respectable. Depending on project priorities (catch more negatives vs. fewer false alarms), I can either keep t\\* = 0.31, re-tune for Macro-F1, or adopt dual thresholds for production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BPS6PadKgqY"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhvo-pKIKgqY"
      },
      "source": [
        "**Goal:** Build a model to automatically detect negative IMDB reviews and reach **F1 ≥ 0.85** on the held-out test set.\n",
        "\n",
        "### What I built\n",
        "- Loaded and cleaned the IMDB dataset; applied **light normalization** (HTML → space, keep apostrophes, lowercase).\n",
        "- Verified **no class imbalance** (≈ 50/50 pos/neg across train/test).\n",
        "- Created multiple text pipelines:\n",
        "  - **Model 0:** Dummy (constant/most frequent) — sanity baseline.\n",
        "  - **Model 1:** `review_norm` → **TF-IDF (1–2 grams)** → **LogisticRegression** (+ NLTK prep variant).\n",
        "  - **Model 3:** **spaCy lemmatization** → TF-IDF (1–2) → LogisticRegression.\n",
        "  - **Model 4:** **spaCy lemmas** → TF-IDF (1–2) → **LightGBM**.\n",
        "  - **Model 9:** **BERT [CLS] embeddings** → LogisticRegression (threshold tuned on validation).\n",
        "\n",
        "### Core results (held-out test)\n",
        "- **Model 0 (Dummy):** Accuracy ≈ 0.50, **F1 = 0.00** — as expected (no skill).\n",
        "- **Model 1 (TF-IDF + LR, NLTK):** **Accuracy = 0.90, F1 = 0.90, APS = 0.96, ROC AUC = 0.97.** ✅ *Meets target.*\n",
        "- **Model 3 (spaCy + TF-IDF + LR):** **Accuracy = 0.89, F1 = 0.89, APS = 0.96, ROC AUC = 0.96.** ✅ *Meets target.*\n",
        "- **Model 4 (spaCy + TF-IDF + LightGBM):** **Accuracy = 0.89, F1 = 0.89, APS = 0.95, ROC AUC = 0.96.** ✅ *Meets target.*\n",
        "- **Model 9 (BERT embeddings + LR, tuned for negatives):**  \n",
        "  Test at tuned threshold **t\\* = 0.31** on *p(negative)*  \n",
        "  – **F1_neg = 0.80, F1_pos ≈ 0.72, Accuracy = 0.77, APS ≈ 0.82, ROC AUC ≈ 0.82.**  \n",
        "  *Good ranking, but below the TF-IDF+LR baselines in F1 on CPU-sized runs.*\n",
        "\n",
        "**Bottom line:** Both **Model 1** and **Model 3** comfortably surpass the **0.85 F1** requirement on the official test set.  \n",
        "Model 4 is close but not clearly better than LR on sparse TF-IDF.  \n",
        "Model 9 is promising but underperforms the linear baselines at this scale/hardware.\n",
        "\n",
        "### “Real” out-of-sample reviews I wrote (mini-set check)\n",
        "- I authored a small set of new reviews with **known labels** to test generalization.\n",
        "- **Model 1** at threshold 0.45: **F1 = 0.75** (Recall strong, a few false positives on sarcasm/nuance).  \n",
        "- **Model 3** at threshold 0.50: **F1 = 0.90** (only one FP and one FN; best robustness on this mini set).  \n",
        "- **Model 4** at 0.50: **F1 = 0.77** with **Recall = 1.00** (caught all positives but flagged too many negatives as positive).\n",
        "\n",
        "**Takeaway from the mini set:** **Model 3 (spaCy + TF-IDF + LR)** handled sarcasm/contrast slightly better and was most balanced on novel text.\n",
        "\n",
        "### Final choice & operating threshold\n",
        "- **Selected model for delivery:** **Model 3 — spaCy + TF-IDF (1–2) + LogisticRegression**  \n",
        "  - **Rationale:** Highest reliability on my out-of-sample reviews, competitive test F1, fast, simple, and easy to maintain.\n",
        "- **Threshold policy:**  \n",
        "  - If the business goal is to **catch more negatives**, use **t ≈ 0.45** on *p(positive)* (or equivalently lower the positive threshold) to improve recall.  \n",
        "  - If you want **balanced precision/recall** and clean confusion matrices, **t = 0.50** is already strong.\n",
        "\n",
        "### Error patterns & what I learned\n",
        "- **Sarcasm/irony, nuanced contrast, and idioms** (e.g., “emotionally empty”, “never a good sign”) are the main sources of mistakes.\n",
        "- Keeping **apostrophes** and **bigrams** is crucial for **negation** (e.g., “not good”, “didn’t like”).\n",
        "\n",
        "### Next steps (if I iterate further)\n",
        "1. **Threshold tuning** on a validation split targeted to the deployment objective (macro-F1 vs. negative-recall).  \n",
        "2. **Light calibration** or **Platt/Isotonic** if well-calibrated probabilities matter.  \n",
        "3. Try **TF-IDF → TruncatedSVD (300–500 dims)** → LR/LightGBM to reduce sparsity and spurious n-grams.  \n",
        "4. If GPU is available, revisit **DistilBERT/BERT** end-to-end fine-tuning (small epochs, gradual scale-up).  \n",
        "5. **Data augmentation** with more sarcastic/contrastive examples to reduce those specific errors.\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusion:** The project objective is achieved.  \n",
        "I recommend shipping **Model 3 (spaCy + TF-IDF + LogisticRegression)** with a documented threshold (0.50 by default, 0.45 if negative-recall is prioritized). It’s accurate (F1 ≈ **0.89–0.90** on test), fast, and interpretable, and it held up best on fresh reviews I wrote.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdnenvFeKgqZ"
      },
      "source": [
        "# Checklist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwsb72WFKgqZ"
      },
      "source": [
        "- [x]  Notebook was opened\n",
        "- [x]  The text data is loaded and pre-processed for vectorization\n",
        "- [x]  The text data is transformed to vectors\n",
        "- [x]  Models are trained and tested\n",
        "- [x]  The metric's threshold is reached\n",
        "- [x]  All the code cells are arranged in the order of their execution\n",
        "- [x]  All the code cells can be executed without errors\n",
        "- [x]  There are conclusions"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
